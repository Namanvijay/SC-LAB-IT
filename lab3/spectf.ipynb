{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import operator\n",
    "from math import e\n",
    "from math import pi\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No']\n"
     ]
    }
   ],
   "source": [
    "spectf=pd.read_csv('SPECTF_test.csv')\n",
    "l=[i for i in range(0,44)]\n",
    "data=spectf.iloc[:,l].values.tolist()\n",
    "y=spectf.iloc[:,44].values.tolist()\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_class(data_list,x):\n",
    "    \n",
    "    group={}\n",
    "    group['Yes']=[]\n",
    "    group['No']=[]\n",
    "    \n",
    "    for i in data_list:\n",
    "        if(y[i]=='Yes'):\n",
    "            group['Yes'].append(data[i])\n",
    "        elif(y[i]=='No'):\n",
    "            group['No'].append(data[i])\n",
    "       \n",
    "    return group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_pdf(x, mean, stdev):\n",
    "        variance = stdev ** 2\n",
    "        exp_squared_diff = (x - mean) ** 2\n",
    "        exp_power = -exp_squared_diff / (2 * variance)\n",
    "        exponent = e ** exp_power\n",
    "        denominator = ((2 * pi) ** .5) * stdev\n",
    "        normal_prob = exponent / denominator\n",
    "        return normal_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_data(data):\n",
    "    return np.mean(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def std_data(data):\n",
    "    return np.std(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_prob(group,target,data):\n",
    "    return len(group[target])/(data)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fun(features):\n",
    "    for feature in zip(*features):\n",
    "            yield {\n",
    "                'stdev': std_data(feature),\n",
    "                'mean': mean_data(feature)\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def joint_prob(row,summaries):\n",
    "        joint_probs = {}\n",
    "        for target, features in summaries.items():\n",
    "            total_features = len(features['summary'])\n",
    "            likelihood = 1\n",
    "            for index in range(total_features):\n",
    "                feature = row[index]\n",
    "                mean = features['summary'][index]['mean']\n",
    "                stdev = features['summary'][index]['stdev']\n",
    "                normal_prob = normal_pdf(feature, mean, stdev)\n",
    "                likelihood *= normal_prob\n",
    "            prob = features['prob']\n",
    "            joint_probs[target] =prob * likelihood\n",
    "        return joint_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(test,summaries):\n",
    "    classes=[]\n",
    "    for i in test:\n",
    "        row=data[i]\n",
    "\n",
    "        joint_probs=joint_prob(row,summaries)\n",
    "        marginal_prob=sum(joint_probs.values())\n",
    "        post_prob={}\n",
    "#         print(joint_probs)\n",
    "#         print(marginal_prob)\n",
    "        for target,prob in joint_probs.items():\n",
    "            post_prob[target]=prob/marginal_prob\n",
    "#         print(post_prob)\n",
    "        x=max(post_prob,key=post_prob.get)\n",
    "       \n",
    "        classes.append(x)\n",
    "    return classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(classes,test_list,x):\n",
    "    corr=0\n",
    "    n=0\n",
    "    test_list=test_list.tolist()\n",
    "\n",
    "    for i in test_list:\n",
    "       \n",
    "        if(y[i]==classes[n]):\n",
    "            corr+=1\n",
    "        n+=1\n",
    "    return corr/len(test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_mat(classes,test_index,x):\n",
    "   \n",
    "    rows,cols=(2,2)\n",
    "    n=0\n",
    "    cm=[[0 for i in range(cols)] for j in range(rows)]\n",
    "    for i in test_index:\n",
    "        if(y[i]=='Yes'):\n",
    "            if(classes[n]=='Yes'):\n",
    "                cm[0][0]+=1\n",
    "            elif(classes[n]=='No'):\n",
    "                cm[0][1]+=1\n",
    "            \n",
    "        elif(y[i]=='No'):\n",
    "            if(classes[n]=='Yes'):\n",
    "                cm[1][0]+=1\n",
    "            elif(classes[n]=='No'):\n",
    "                cm[1][1]+=1\n",
    "           \n",
    "        n+=1\n",
    "        \n",
    "    print(\"  Confusion Matrix\")\n",
    "    print(cm)\n",
    "    if(cm[0][0]+cm[0][1]==0):\n",
    "        if(cm[0][0]==0 and cm[0][1]==0 and cm[1][0]==0):\n",
    "            print(\"  Sensitivity is {}\".format(1.0))\n",
    "        else:\n",
    "            print(\"  Sensitivity is {}\".format(0.0))\n",
    "    else:\n",
    "        \n",
    "        sensitivity=cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "        print(\"  Sensitivity is {}\".format(sensitivity))\n",
    "    if(cm[1][1]+cm[1][0]==0):\n",
    "        if(cm[0][1]==0):\n",
    "              print(\"  Specificity is {}\".format(1.0))\n",
    "        else:\n",
    "             print(\"  Specificity is {}\".format(0.0))\n",
    "    else:\n",
    "        spec=cm[1][1]/(cm[1][1]+cm[1][0])\n",
    "        print(\"  Specificity is {}\".format(spec))\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 fold States------------->\n",
      "  Confusion Matrix\n",
      "[[10, 1], [0, 0]]\n",
      "  Sensitivity is 0.9090909090909091\n",
      "  Specificity is 0.0\n",
      "  Accuracy is 90.9090909090909\n",
      "2 fold States------------->\n",
      "  Confusion Matrix\n",
      "[[1, 3], [2, 5]]\n",
      "  Sensitivity is 0.25\n",
      "  Specificity is 0.7142857142857143\n",
      "  Accuracy is 54.54545454545454\n",
      "3 fold States------------->\n",
      "  Confusion Matrix\n",
      "[[1, 2], [1, 7]]\n",
      "  Sensitivity is 0.3333333333333333\n",
      "  Specificity is 0.875\n",
      "  Accuracy is 72.72727272727273\n",
      "4 fold States------------->\n",
      "  Confusion Matrix\n",
      "[[3, 8], [0, 0]]\n",
      "  Sensitivity is 0.2727272727272727\n",
      "  Specificity is 0.0\n",
      "  Accuracy is 27.27272727272727\n",
      "5 fold States------------->\n",
      "  Confusion Matrix\n",
      "[[7, 4], [0, 0]]\n",
      "  Sensitivity is 0.6363636363636364\n",
      "  Specificity is 0.0\n",
      "  Accuracy is 63.63636363636363\n",
      "6 fold States------------->\n",
      "  Confusion Matrix\n",
      "[[7, 4], [0, 0]]\n",
      "  Sensitivity is 0.6363636363636364\n",
      "  Specificity is 0.0\n",
      "  Accuracy is 63.63636363636363\n",
      "7 fold States------------->\n",
      "  Confusion Matrix\n",
      "[[4, 0], [0, 7]]\n",
      "  Sensitivity is 1.0\n",
      "  Specificity is 1.0\n",
      "  Accuracy is 100.0\n",
      "8 fold States------------->\n",
      "  Confusion Matrix\n",
      "[[0, 0], [0, 11]]\n",
      "  Sensitivity is 1.0\n",
      "  Specificity is 1.0\n",
      "  Accuracy is 100.0\n",
      "9 fold States------------->\n",
      "  Confusion Matrix\n",
      "[[0, 0], [1, 10]]\n",
      "  Sensitivity is 0.0\n",
      "  Specificity is 0.9090909090909091\n",
      "  Accuracy is 90.9090909090909\n",
      "10 fold States------------->\n",
      "  Confusion Matrix\n",
      "[[0, 0], [2, 9]]\n",
      "  Sensitivity is 0.0\n",
      "  Specificity is 0.8181818181818182\n",
      "  Accuracy is 81.81818181818183\n",
      "\n",
      "\n",
      "      Average Accuracy of the model is 74.54545454545453\n"
     ]
    }
   ],
   "source": [
    "kf=KFold(n_splits=10, shuffle=False)\n",
    "acc_mean=[]\n",
    "x=0\n",
    "for train_index, test_index in kf.split(spectf):\n",
    "\n",
    "    group=group_class(train_index,x)\n",
    "\n",
    "    summaries = {}\n",
    "    for target,features in group.items():\n",
    "        summaries[target]={\n",
    "            'prob':class_prob(group, target, len(train_index)),\n",
    "            'summary': [i for i in fun(features)],\n",
    "        }\n",
    "    \n",
    "\n",
    "    test=group_class(test_index,x)\n",
    "    classes=predict(test_index,summaries)\n",
    "    \n",
    "        \n",
    "   \n",
    "    print(\"{} fold States------------->\".format(x+1))\n",
    "    acc=accuracy(classes,test_index,x)\n",
    "    acc_mean.append(acc)\n",
    "    confusion_mat(classes,test_index,x)\n",
    "    \n",
    "    print(\"  Accuracy is {}\".format(acc*100))\n",
    "    \n",
    "    x+=1\n",
    "print(\"\\n\")\n",
    "    \n",
    "print(\"      Average Accuracy of the model is {}\".format(np.mean(acc_mean)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
